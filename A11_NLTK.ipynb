{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\abres\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\abres\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ralph spoke to Mary about the party.', 'Mary had visited Gogo to ask him about the party.', 'Gogo had danced with Yuki the night before and was tired.', 'Yuki asked Zelda to join her.', 'Zelda often spoke to Yuki.', 'Zelda was visiting Zizhen.', 'Zizhen was talking with Ralph about the party.', 'Ralph also said he had danced with Yuki but that she moved too fast.']\n"
     ]
    }
   ],
   "source": [
    "from nltk import sent_tokenize\n",
    "import string\n",
    "\n",
    "# Read and split into phrases\n",
    "with open('party.txt') as file:\n",
    "    text = file.read()\n",
    "    \n",
    "phrases = sent_tokenize(text)\n",
    "\n",
    "print(phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "\n",
    "tag_list = []\n",
    "for i in phrases:\n",
    "    words = word_tokenize(i)\n",
    "    word_tags = nltk.pos_tag(words)\n",
    "    tags = []\n",
    "    for j in word_tags:\n",
    "        tags.append(j[1])\n",
    "    tag_list.append(tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NNP VBD TO NNP\n",
      "NNP VBD VBN NNP\n",
      "NNP VBD VBN IN NNP\n",
      "NNP VBD NNP\n",
      "NNP RB VBD TO NNP\n",
      "NNP VBD VBG NNP\n",
      "NNP VBD VBG IN NNP\n",
      "NNP RB VBD PRP VBD VBN IN NNP\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "searches = []\n",
    "for i in tag_list:\n",
    "    sentence = ' '.join(i)\n",
    "    search = re.findall('.*?NNP.*?V..?.*?NNP',sentence)\n",
    "    if search:\n",
    "        print(search[0])\n",
    "        searches.append(search[0])\n",
    "    else:\n",
    "        searches.append(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NNP VBD TO NNP\n",
      "Length: 4\n",
      "Ralph spoke to Mary\n",
      "[('Ralph', 'NNP'), ('spoke', 'VBD'), ('to', 'TO'), ('Mary', 'NNP')]\n",
      "\n",
      "\n",
      "NNP VBD VBN NNP\n",
      "Length: 4\n",
      "Mary had visited Gogo\n",
      "[('Mary', 'NNP'), ('had', 'VBD'), ('visited', 'VBN'), ('Gogo', 'NNP')]\n",
      "\n",
      "\n",
      "NNP VBD VBN IN NNP\n",
      "Length: 5\n",
      "Gogo had danced with Yuki\n",
      "[('Gogo', 'NNP'), ('had', 'VBD'), ('danced', 'VBN'), ('with', 'IN'), ('Yuki', 'NNP')]\n",
      "\n",
      "\n",
      "NNP VBD NNP\n",
      "Length: 3\n",
      "Yuki asked Zelda\n",
      "[('Yuki', 'NNP'), ('asked', 'VBD'), ('Zelda', 'NNP')]\n",
      "\n",
      "\n",
      "NNP RB VBD TO NNP\n",
      "Length: 5\n",
      "Zelda often spoke to Yuki.\n",
      "[('Zelda', 'NNP'), ('often', 'RB'), ('spoke', 'VBD'), ('to', 'TO'), ('Yuki.', 'NNP')]\n",
      "\n",
      "\n",
      "NNP VBD VBG NNP\n",
      "Length: 4\n",
      "Zelda was visiting Zizhen.\n",
      "[('Zelda', 'NNP'), ('was', 'VBD'), ('visiting', 'VBG'), ('Zizhen.', 'NNP')]\n",
      "\n",
      "\n",
      "NNP VBD VBG IN NNP\n",
      "Length: 5\n",
      "Zizhen was talking with Ralph\n",
      "[('Zizhen', 'NNP'), ('was', 'VBD'), ('talking', 'VBG'), ('with', 'IN'), ('Ralph', 'NNP')]\n",
      "\n",
      "\n",
      "NNP RB VBD PRP VBD VBN IN NNP\n",
      "Length: 8\n",
      "Ralph also said he had danced with Yuki\n",
      "[('Ralph', 'NNP'), ('also', 'RB'), ('said', 'VBD'), ('he', 'PRP'), ('had', 'VBD'), ('danced', 'VBN'), ('with', 'IN'), ('Yuki', 'NNP')]\n",
      "\n",
      "\n",
      "[('Ralph', 'NNP'), ('spoke', 'VBD'), ('to', 'TO'), ('Mary', 'NNP')]\n",
      "[('Mary', 'NNP'), ('had', 'VBD'), ('visited', 'VBN'), ('Gogo', 'NNP')]\n",
      "[('Gogo', 'NNP'), ('had', 'VBD'), ('danced', 'VBN'), ('with', 'IN'), ('Yuki', 'NNP')]\n",
      "[('Yuki', 'NNP'), ('asked', 'VBD'), ('Zelda', 'NNP')]\n",
      "[('Zelda', 'NNP'), ('often', 'RB'), ('spoke', 'VBD'), ('to', 'TO'), ('Yuki.', 'NNP')]\n",
      "[('Zelda', 'NNP'), ('was', 'VBD'), ('visiting', 'VBG'), ('Zizhen.', 'NNP')]\n",
      "[('Zizhen', 'NNP'), ('was', 'VBD'), ('talking', 'VBG'), ('with', 'IN'), ('Ralph', 'NNP')]\n",
      "[('Ralph', 'NNP'), ('also', 'RB'), ('said', 'VBD'), ('he', 'PRP'), ('had', 'VBD'), ('danced', 'VBN'), ('with', 'IN'), ('Yuki', 'NNP')]\n"
     ]
    }
   ],
   "source": [
    "# This is some of the ugliest garbage i've ever written but it works\n",
    "joined_phrases = []\n",
    "for i in range(len(phrases)):\n",
    "    print(searches[i])\n",
    "    split_searches = searches[i].split()\n",
    "    length = len(searches[i].split())\n",
    "    \n",
    "    if length > 0:\n",
    "        print(\"Length:\",length)\n",
    "        phrase = phrases[i]\n",
    "        split = phrase.split()\n",
    "        print(' '.join(split[0:length]))\n",
    "        joined_phrase_list = []\n",
    "        for j in range(length):\n",
    "            joined_phrase_list.append((split[j],split_searches[j]))\n",
    "        print(joined_phrase_list)\n",
    "        joined_phrases.append(joined_phrase_list)\n",
    "        print(\"\\n\")\n",
    "        \n",
    "for i in joined_phrases:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "People: ['Mary', 'Ralph', 'Gogo', 'Yuki', 'Zizhen', 'Zelda']\n",
      "('Ralph', 'spoke', 'Mary')\n",
      "('Mary', 'visited', 'Gogo')\n",
      "('Gogo', 'danced', 'Yuki')\n",
      "('Yuki', 'asked', 'Zelda')\n",
      "('Zelda', 'spoke', 'Yuki')\n",
      "('Zelda', 'visiting', 'Zizhen')\n",
      "('Zizhen', 'talking', 'Ralph')\n",
      "('Ralph', 'danced', 'Yuki')\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "people = []\n",
    "triples = []\n",
    "\n",
    "ps = PorterStemmer()\n",
    "knows_words = ['talk','ask','speak','spoke','danc','visit']\n",
    "for sentence in joined_phrases:\n",
    "    for word_pair in sentence:\n",
    "        if re.match('V..?',word_pair[1]):\n",
    "            if ps.stem(word_pair[0]) in knows_words:\n",
    "                temp_people=[]\n",
    "                for i in sentence:\n",
    "                    if i[1] == 'NNP':\n",
    "                        no_period_string = re.sub('\\.','',i[0])\n",
    "                        temp_people.append(no_period_string)\n",
    "                        people.append(no_period_string)\n",
    "                triples.append((temp_people[0], word_pair[0], temp_people[1]))\n",
    "\n",
    "people = list(set(people))\n",
    "print(\"People:\",people)\n",
    "for i in triples:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"mydata.n3\",'w') as outfile:\n",
    "    outfile.write(\"\"\"@prefix :  <http://www.lyle.smu.edu//#> .\n",
    "@prefix rdf:  <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .\n",
    "@prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> .\n",
    "@prefix xsd: <http://www.w3.org/2001/XMLSchema#>.\n",
    "@prefix schema: <http://schema.org/> .\\n\\n\"\"\")\n",
    "    for i in people:\n",
    "        outstring = \":\" + i + \" rdf:type schema:Person .\\n\"\n",
    "        outfile.write(outstring)\n",
    "    outfile.write(\"\\n\")\n",
    "    for i in triples:\n",
    "        outstring = \":\" + i[0] + \" schema:knows :\" + i[2] + \" .\\n\"\n",
    "        outfile.write(outstring)\n",
    "        # outstring = \":\" + i[2] + \" schema:knows :\" + i[0] + \" .\\n\"\n",
    "        # outfile.write(outstring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
